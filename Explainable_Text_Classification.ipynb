{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Download data"
      ],
      "metadata": {
        "id": "EjY7GCNBRTnn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j0ku90oIUw0c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4954da0-4a06-418e-9d49-60455d868286"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "84131840/84125825 [==============================] - 14s 0us/step\n",
            "84140032/84125825 [==============================] - 14s 0us/step\n"
          ]
        }
      ],
      "source": [
        "# Based on https://www.tensorflow.org/tutorials/keras/text_classification\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
        "\n",
        "dataset = tf.keras.utils.get_file(\"aclImdb_v1\", url,  untar=True, cache_dir='.', cache_subdir='')\n",
        "\n",
        "dataset_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "fIkNHi0ZeZ0e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Remove additional folders"
      ],
      "metadata": {
        "id": "NtitdCTjRXGK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EyF4i_mEgl8b"
      },
      "outputs": [],
      "source": [
        "remove_dir = os.path.join(dataset_dir + '/train/', 'unsup')\n",
        "shutil.rmtree(remove_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Load data from folders"
      ],
      "metadata": {
        "id": "lzLrHnTWRb7m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pWUcrzqIZ819"
      },
      "outputs": [],
      "source": [
        "train_ds = tf.keras.utils.text_dataset_from_directory(\n",
        "    'aclImdb/train', \n",
        "    batch_size=32, \n",
        "    validation_split=0.2, \n",
        "    subset='training', seed=111)\n",
        "\n",
        "val_ds = tf.keras.utils.text_dataset_from_directory(\n",
        "    'aclImdb/train', \n",
        "    batch_size=32, \n",
        "    validation_split=0.2, \n",
        "    subset='validation', seed=111)\n",
        "\n",
        "test_ds = tf.keras.utils.text_dataset_from_directory(\n",
        "    'aclImdb/test', \n",
        "    batch_size=32)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "xXcqxnYsgCbA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. View data"
      ],
      "metadata": {
        "id": "pXfRZH1CRnA9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pMk6sRWBXMCd"
      },
      "outputs": [],
      "source": [
        "for x, y in train_ds.take(1):\n",
        "  print(y, x)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Preprocess text\n",
        "\n"
      ],
      "metadata": {
        "id": "hcB0xDwfRiXv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yOtKIcQPWNla"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import string \n",
        "\n",
        "def custom_standardization(input_data):\n",
        "  lowercase = tf.strings.lower(input_data)\n",
        "  return tf.strings.regex_replace(lowercase, '[%s]' % re.escape(string.punctuation), '')\n",
        "\n",
        "max_features = 10000\n",
        "sequence_length = 250\n",
        "\n",
        "vectorize_layer = tf.keras.layers.TextVectorization(\n",
        "    standardize=custom_standardization,\n",
        "    max_tokens=max_features,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=sequence_length)\n",
        "\n",
        "train_text = train_ds.map(lambda x, y: x)\n",
        "vectorize_layer.adapt(train_text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = tf.constant(['Hello World'])\n",
        "print(vectorize_layer(text))"
      ],
      "metadata": {
        "id": "iS4-MBpABm6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Create and train"
      ],
      "metadata": {
        "id": "Ao3UiiclRqYY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AgIkMJ52XOeP"
      },
      "outputs": [],
      "source": [
        "inputs = tf.keras.layers.Input((1,), dtype=tf.string)\n",
        "embedding_layer = tf.keras.layers.Embedding(max_features + 1, 100, trainable=True)\n",
        "\n",
        "x = vectorize_layer(inputs)\n",
        "x = embedding_layer(x)\n",
        "\n",
        "conv1 = tf.keras.layers.Conv1D(100, kernel_size=2)(x)\n",
        "conv1 = tf.keras.layers.Dropout(0.4)(conv1)\n",
        "conv1 = tf.keras.layers.GlobalMaxPool1D()(conv1)\n",
        "\n",
        "conv2 = tf.keras.layers.Conv1D(100, kernel_size=3)(x)\n",
        "conv2 = tf.keras.layers.Dropout(0.4)(conv2)\n",
        "conv2 = tf.keras.layers.GlobalMaxPool1D()(conv2)\n",
        "\n",
        "conv3 = tf.keras.layers.Conv1D(100, kernel_size=4)(x)\n",
        "conv3 = tf.keras.layers.Dropout(0.4)(conv3)\n",
        "conv3 = tf.keras.layers.GlobalMaxPool1D()(conv3)\n",
        "\n",
        "conv4 = tf.keras.layers.Conv1D(100, kernel_size=5)(x)\n",
        "conv4 = tf.keras.layers.Dropout(0.4)(conv4)\n",
        "conv4 = tf.keras.layers.GlobalMaxPool1D()(conv4)\n",
        "\n",
        "outputs = tf.keras.layers.Add()([conv1, conv2, conv3, conv4])\n",
        "outputs = tf.keras.layers.Dense(100, activation='relu')(outputs)\n",
        "outputs = tf.keras.layers.Dense(75, activation='relu')(outputs)\n",
        "outputs = tf.keras.layers.Dense(50, activation='relu')(outputs)\n",
        "outputs = tf.keras.layers.Dense(1, activation = 'sigmoid')(outputs)\n",
        "\n",
        "model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "model.summary()\n",
        "model.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(), metrics=tf.metrics.BinaryAccuracy())\n",
        "epochs = 3\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Evaluate model"
      ],
      "metadata": {
        "id": "9rdmQ3ASRvxS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wN5sVgufadD6"
      },
      "outputs": [],
      "source": [
        "loss, accuracy = model.evaluate(test_ds)\n",
        "\n",
        "print(\"Loss: \", loss)\n",
        "print(\"Accuracy: \", accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = tf.constant(['This movie was really funny'])\n",
        "model(text)"
      ],
      "metadata": {
        "id": "BtONmF4qjU8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Calculate the gradient of embedded_text with respect to the output"
      ],
      "metadata": {
        "id": "t5j5VErYR043"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yoaO5v-fah0c"
      },
      "outputs": [],
      "source": [
        "embedding_layer = model.layers[2]\n",
        "new_model = tf.keras.Model(inputs=model.layers[3].input, outputs=model.layers[-1].output)\n",
        "\n",
        "#text = tf.constant(['I love this film. It is well written and acted and has good cinematography'])\n",
        "#text = tf.constant([\"Let me start off by saying that this doesn't seem or feel like a movie\"])\n",
        "#text = tf.constant([\"I didn't enjoy this film. I thought the acting wasn't very good and the story was boring.\"])\n",
        "#text = tf.constant([\"I'm not going to say that this movie is horrible, because I have seen worse, but it's not even halfway decent.\"])\n",
        "#text = tf.constant([\"This movie was just horrible\"])\n",
        "#text = tf.constant([\"This was a very dull but enjoyable movie\"])\n",
        "#text = tf.constant([\"This movie was a disgrace\"])\n",
        "\n",
        "with tf.GradientTape(persistent=True) as tape:\n",
        "    x = vectorize_layer(text)\n",
        "    embedded_text = embedding_layer(x)\n",
        "    tape.watch(embedded_text)\n",
        "    y = new_model(embedded_text)\n",
        "    print(y)\n",
        "\n",
        "grads = tape.gradient(y, embedded_text)\n",
        "\n",
        "text = tf.strings.split(text)\n",
        "text = [x.decode('utf-8') for x in text.to_tensor().numpy()[0]]\n",
        "output = tf.squeeze(tf.reduce_max(tf.abs(grads), axis=-1, keepdims=True)[:, :len(text), :], axis=-1)[0].numpy()\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Visualize results"
      ],
      "metadata": {
        "id": "Eer8kD_ISTQo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OUVRBJHAnAcq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from matplotlib.ticker import FormatStrFormatter\n",
        "\n",
        "sns.set_style(\"whitegrid\", {\"grid.color\": \".98\"})\n",
        "\n",
        "def gradientbars(bars, cmap):\n",
        "    ax = bars[0].axes\n",
        "    xmin, xmax = ax.get_xlim()\n",
        "    ymin, ymax = ax.get_ylim()\n",
        "    for bar in bars:\n",
        "        bar.set_zorder(1)\n",
        "        bar.set_facecolor(\"none\")\n",
        "        x, y = bar.get_xy()\n",
        "        w, h = bar.get_width(), bar.get_height()\n",
        "        grad = np.linspace(y, y + h, 256).reshape(256, 1)\n",
        "        ax.imshow(grad, extent=[x, x + w, y, y + h], aspect=\"auto\", zorder=0, origin='lower',\n",
        "                  vmin=ymin, vmax=ymax, cmap=cmap)\n",
        "    ax.axis([xmin, xmax, ymin, ymax])\n",
        "\n",
        "def erase_xaxis(ax):\n",
        "    ax.tick_params(\n",
        "    axis='x',         \n",
        "    which='both',      \n",
        "    bottom=False,      \n",
        "    top=False,        \n",
        "    labelbottom=False)\n",
        "    \n",
        "def add_text(rects, text, ax):\n",
        "    for idx, rect in enumerate(rects):\n",
        "        height = rect.get_height()\n",
        "        ax.text(rect.get_x()+rect.get_width()/2., 1.01*height, text[idx],\n",
        "                ha='center', va='bottom', size=11)\n",
        "    \n",
        "fig, ax1 = plt.subplots(1, 1, figsize=(7, 7), dpi=80)\n",
        "fig.tight_layout()\n",
        "fig.subplots_adjust(hspace=0.3, wspace=0.5)\n",
        "\n",
        "ax1.set_title('Text w.r.t  \\noutput probability:{:f}'.format(y[0][0]))\n",
        "rects = ax1.bar(np.arange(len(output)), output)\n",
        "gradientbars(rects, 'Blues')\n",
        "erase_xaxis(ax1)\n",
        "add_text(rects, text, ax1)\n",
        "ax1.ticklabel_format(style='plain', axis='y')\n",
        "ax1.yaxis.set_major_formatter(FormatStrFormatter('%.3f'))\n",
        "\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Explainable_Text_Classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}